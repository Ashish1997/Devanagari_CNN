{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ashish\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Shuffling data...\n",
      "Scaling Features...\n",
      "Splitting dataset...\n",
      "The image is character:  à¤°\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Set size:  73600\n",
      "Test Set size:  18400\n",
      "(18400, 1024)\n",
      "(18400, 1)\n",
      "(10000, 1024)\n",
      "x_train shape: (10000, 32, 32, 1)\n",
      "10000 train samples\n",
      "18400 test samples\n",
      "0\n",
      "y_train[0]= [19]\n",
      "WARNING:tensorflow:From C:\\Users\\Ashish\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1255: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# check the loss function again\\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\\nmodel.compile(loss=keras.losses.categorical_crossentropy,\\n              optimizer=keras.optimizers.sgd(),\\n              metrics=[\\'accuracy\\'])\\nmodel.fit(x_train, y_train,\\n              batch_size=batch_size,\\n              epochs=epochs,\\n              verbose=1,\\n              validation_data=(x_test, y_test))\\nmodel_json = model.to_json()\\nwith open(\"model.json\", \"w\") as json_file:\\n    json_file.write(model_json)\\n\\n\\njson_file = open(\\'model.json\\', \\'r\\')\\nloaded_model_json = json_file.read()\\n\\n#model.save_weights(\"model.h5\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import cv2\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from characters import characters\n",
    "from keras.models import model_from_json\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "verbose = True\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def load_data():\n",
    "    if verbose:\n",
    "        print('Loading dataset...')\n",
    "    m = 92000\n",
    "    data = np.load('dataset/dataset.npz')\n",
    "\n",
    "    x_train = data['arr_0']\n",
    "    y_train = data['arr_1'].reshape(78200, 1)\n",
    "    x_test = data['arr_2']\n",
    "    y_test = data['arr_3'].reshape(m - 78200, 1)\n",
    "\n",
    "    X = np.vstack([x_train, x_test]).reshape(m, 1024)\n",
    "    Y = np.vstack([y_train, y_test]).reshape(m, 1)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def shuffle(X, Y):\n",
    "    if verbose:\n",
    "        print('Shuffling data...')\n",
    "    from sklearn.utils import shuffle\n",
    "    X, Y = shuffle(X, Y)\n",
    "    return X, Y\n",
    "\n",
    "def plot(X, Y, n = 0):\n",
    "    print('The image is character: ', characters[Y[n][0] - 1])\n",
    "    plt.imshow(X[n, :].reshape(32, 32), cmap = 'Greys')\n",
    "    plt.show()\n",
    "\n",
    "def scale(X, factor = 255):\n",
    "    if verbose:\n",
    "        print('Scaling Features...')\n",
    "    return X*(1/255)\n",
    "\n",
    "def split(x, y, ratio = 0.2):\n",
    "    if verbose:\n",
    "        print('Splitting dataset...')\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    return train_test_split(x, y, test_size = ratio)\n",
    "\n",
    "def main():\n",
    "    X, Y = load_data()\n",
    "    X, Y = shuffle(X, Y)\n",
    "    X = scale(X)\n",
    "    x_train, x_test, y_train, y_test = split(X, Y)\n",
    "\n",
    "    m_train = y_train.shape[0]\n",
    "    m_test =y_test.shape[0]\n",
    "    plot(X,Y,1)\n",
    "    \n",
    "\n",
    "    print('\\nTraining Set size: ', m_train)\n",
    "    print('Test Set size: ', m_test)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "x_train_full, x_test_full, y_train_full, y_test_full = main()\n",
    "\n",
    "img_rows, img_cols = 32,32\n",
    "#print(y_test)\n",
    "\n",
    "x_train = x_test_full[0:10000]\n",
    "y_train = y_test_full[0:10000] - 1\n",
    "x_test = x_test_full\n",
    "y_test = y_test_full - 1\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 46\n",
    "epochs = 30\n",
    "\n",
    "print(x_train.shape)    \n",
    "    \n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print(np.min(y_train))\n",
    "print(\"y_train[0]=\",y_train[0])\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "    \n",
    "model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                     strides = (1,1), \n",
    "                     activation='relu',\n",
    "                     padding='same',\n",
    "                     kernel_initializer= keras.initializers.RandomNormal(mean=0.0, stddev=0.1, seed=None),\n",
    "                     input_shape=input_shape,\n",
    "                     name = 'conv01'));\n",
    "model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                     strides = (1,1), \n",
    "                     activation='relu',\n",
    "                     padding='same',\n",
    "                     kernel_initializer= keras.initializers.RandomNormal(mean=0.0, stddev=0.1, seed=None),\n",
    "                     name = 'conv02'));\n",
    "    \n",
    "model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                     strides = (1,1), \n",
    "                     activation='relu',\n",
    "                     padding='same',\n",
    "                     kernel_initializer= keras.initializers.RandomNormal(mean=0.0, stddev=0.1, seed=None),\n",
    "                     name = 'conv03'));\n",
    "    \n",
    "model.add(MaxPooling2D((2, 2), strides = (2,2), name='max_pool'))\n",
    "    \n",
    "model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                     strides = (1,1), \n",
    "                     activation='relu',\n",
    "                     padding='same',\n",
    "                     kernel_initializer= keras.initializers.RandomNormal(mean=0.0, stddev=0.1, seed=None),\n",
    "                     name = 'conv11'));\n",
    "    \n",
    "model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                     strides = (1,1), \n",
    "                     activation='relu',\n",
    "                     padding='same',\n",
    "                     kernel_initializer= keras.initializers.RandomNormal(mean=0.0, stddev=0.1, seed=None),\n",
    "                     name = 'conv12'));\n",
    "    \n",
    "model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                     strides = (1,1), \n",
    "                     activation='relu',\n",
    "                     padding='same',\n",
    "                     kernel_initializer= keras.initializers.RandomNormal(mean=0.0, stddev=0.1, seed=None),\n",
    "                     name = 'conv13'));\n",
    "    \n",
    "model.add(MaxPooling2D((2, 2), strides = (2,2), name='max_pool1'))\n",
    "    \n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "'''\n",
    "# check the loss function again\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.sgd(),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test))\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "\n",
    "#model.save_weights(\"model.h5\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#json_file.close()\n",
    "#loaded_model = model_from_json(loaded_model_json)\n",
    "y_train[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Ashish\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2857: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\Ashish\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1340: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "18400/18400 [==============================] - 239s 13ms/step\n",
      "Test loss: 0.11496986867532066\n",
      "Test accuracy: 0.9746195652173913\n",
      "18400/18400 [==============================] - 239s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.sgd(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "y_cnn = model.predict_classes(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('CNN_out.txt', 'w') as f:\n",
    "    f.write(str(y_cnn+1))  # Python 3.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18400, 1024)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
